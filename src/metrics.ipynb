{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Metrics"
      ],
      "metadata": {
        "id": "dKSd8ZepjF4q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Metrics calculations is as following:"
      ],
      "metadata": {
        "id": "P6M9HtJ5jIVC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.metrics import roc_auc_score, roc_curve\n",
        "import matplotlib.pyplot as plt\n"
      ],
      "metadata": {
        "id": "_ykgPCF0jELh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0hPnWtBOjCVy"
      },
      "outputs": [],
      "source": [
        "#loading the BMA file\n",
        "bma_file = '/content/drive/My Drive/colab_output/bma_file_example.txt'\n",
        "bma_df = pd.read_csv(bma_file, sep='\\t')\n",
        "\n",
        "#positives loaded here\n",
        "positive_file = '/content/drive/My Drive/positives_added.txt' #fold-based or overall (depending if graph contains the query or not)\n",
        "positive_df = pd.read_csv(positive_file, sep='\\t')\n",
        "\n",
        "#cleaning and pre-processing\n",
        "bma_df['MGI'] = bma_df['MGI Entity'].str.extract(r'MGI_(\\d+)')\n",
        "bma_df['OMIM'] = bma_df['OMIM Entity'].str.extract(r'OMIM_(\\d+)')\n",
        "positive_df['MGI'] = positive_df['MGI_ID'].str.extract(r'MGI_(\\d+)')\n",
        "positive_df['OMIM'] = positive_df['OMIM_ID'].str.extract(r'OMIM_(\\d+)')\n",
        "\n",
        "#labelling\n",
        "merged_df = pd.merge(bma_df, positive_df[['MGI', 'OMIM']], on=['MGI', 'OMIM'], how='left', indicator=True)\n",
        "merged_df['y_true'] = merged_df['_merge'].apply(lambda x: 1 if x == 'both' else 0)\n",
        "merged_df['y_scores'] = merged_df['BMA Similarity']\n",
        "\n",
        "#auc computation\n",
        "auc = roc_auc_score(merged_df['y_true'], merged_df['y_scores'])\n",
        "print(f\"AUC Score: {auc}\")\n",
        "\n",
        "#--calculations needed\n",
        "#hits@k, MR, and MRR calculation\n",
        "hits_at_1 = 0\n",
        "hits_at_10 = 0\n",
        "hits_at_30 = 0\n",
        "ranks = []\n",
        "\n",
        "grouped = merged_df.groupby('OMIM')\n",
        "\n",
        "for omim_id, group in grouped:\n",
        "    sorted_group = group.sort_values(by='y_scores', ascending=False).reset_index(drop=True)\n",
        "    positive_indices = sorted_group.index[sorted_group['y_true'] == 1].tolist()\n",
        "\n",
        "    if not positive_indices:\n",
        "        continue\n",
        "\n",
        "\n",
        "    rank = positive_indices[0] + 1\n",
        "    ranks.append(rank)\n",
        "\n",
        "    if rank == 1:\n",
        "        hits_at_1 += 1\n",
        "    if rank <= 10:\n",
        "        hits_at_10 += 1\n",
        "    if rank <= 30:\n",
        "        hits_at_30 += 1\n",
        "\n",
        "total = len(ranks)\n",
        "\n",
        "#final metrics\n",
        "mean_rank = sum(ranks) / total\n",
        "mrr = sum(1.0 / r for r in ranks) / total\n",
        "hits_at_1_score = hits_at_1 / total\n",
        "hits_at_10_score = hits_at_10 / total\n",
        "hits_at_30_score = hits_at_30 / total\n",
        "\n",
        "# display results\n",
        "print(f\"Hits@1: {hits_at_1_score:.4f}\")\n",
        "print(f\"Hits@10: {hits_at_10_score:.4f}\")\n",
        "print(f\"Hits@30: {hits_at_30_score:.4f}\")\n",
        "print(f\"Mean Rank: {mean_rank:.2f}\")\n",
        "print(f\"MRR: {mrr:.4f}\")\n"
      ]
    }
  ]
}